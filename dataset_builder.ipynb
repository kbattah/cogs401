{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eef87459-cad9-47e3-b3b0-cc18d14a7591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "import pandas as pd\n",
    "\n",
    "#GLOBAL VARS\n",
    "maxResults = 20 #want 20 videos per keyword\n",
    "category = [] # Data to be stored\n",
    "\n",
    "# Gathering Data using the yt API, have many because daily quota easy to exceed\n",
    "API_KEY1 = \"AIzaSyAh1sVjvm7b7GH0_W-1AXYOFldVvgRM9Gk\"\n",
    "API_KEY2 = \"AIzaSyAfIe8O946wN41Bla1JnRAGooASlNWMX1I\"\n",
    "API_KEY3 = \"AIzaSyAUZXU9m9WdIxRFHAdK2lBbCtwgFQPO-tI\"\n",
    "API_KEY4 = \"AIzaSyAWPuynMfLH1u0M_tOPytdqXHuLo80Yjeo\"\n",
    "API_KEY5 = \"AIzaSyD5igxO2TntNPXzaWe2EljzbRzATx_vRf8\"\n",
    "API_KEY6 =\"AIzaSyCBLJcm3WV7n-de3tns92yA9qTF8BZUvfQ\"\n",
    "API_KEY7 = \"AIzaSyAS9eTgOEnOJ2GlJbbqm_0bR1onuRQjTHE\"\n",
    "API_KEY8 = \"AIzaSyAg6dxTbCfUlEFyKffbdQ-JsrddL90uZus\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40356e03-a78a-4dda-8292-d78007293b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(id, api_key):\n",
    "    \n",
    "    # retrieve youtube video results\n",
    "    comments = \"\"\n",
    "    replies = \"\"\n",
    "    youtube = build('youtube', 'v3',\n",
    "                    developerKey=api_key)\n",
    "    try:\n",
    "        video_response=youtube.commentThreads().list(\n",
    "            part='snippet,replies',\n",
    "            videoId=id\n",
    "        ).execute()\n",
    "\n",
    "        while video_response:\n",
    "            for item in video_response['items']:\n",
    "                comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "                # get num of reply of comment\n",
    "                replycount = item['snippet']['totalReplyCount']\n",
    "                # if reply is there, get it\n",
    "                if replycount>0:\n",
    "                    for reply in item['replies']['comments']:\n",
    "                        reply = reply['snippet']['textDisplay']\n",
    "                        replies += reply\n",
    "                # add comment to big comment string, along with its replies\n",
    "                comments += \" \" + comment +\" \"+ replies\n",
    "                # empty reply string\n",
    "                replies = \"\"\n",
    "\n",
    "            # repeat for next page\n",
    "            if 'nextPageToken' in video_response:\n",
    "                video_response = youtube.commentThreads().list(\n",
    "                    part = 'snippet,replies',\n",
    "                    videoId = id,\n",
    "                    pageToken = video_response['nextPageToken']\n",
    "                ).execute()\n",
    "            else:\n",
    "                break \n",
    "        return comments\n",
    "    except: #returns error if comments have been disabled, empty string is like video with 0 comments\n",
    "        return \" \"\n",
    "   \n",
    "    \n",
    "# Allows to get complete description (more than 160 chars)\n",
    "def get_full_desc(id, api_key):\n",
    "    \"\"\"\n",
    "    Get complete description of a YouTube video.\n",
    "\n",
    "    Parameters:\n",
    "    - id (str): The video ID.\n",
    "    - api_key (str): The YouTube Data API key.\n",
    "\n",
    "    Returns:\n",
    "    str: The complete description of the video.\n",
    "    \"\"\"\n",
    "\n",
    "    scopes = [\"https://www.googleapis.com/auth/youtube.readonly\"]\n",
    "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    request = youtube.videos().list(\n",
    "        part=\"snippet\",\n",
    "        id=id\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    # access description from snippet\n",
    "    description = response['items'][0]['snippet']['description']\n",
    "\n",
    "    return description\n",
    "\n",
    "#Reads all keywords from label file\n",
    "def read_keywords_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Read keywords from a file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The path to the file containing keywords.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of keywords.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        keywords = [line.strip() for line in file.readlines()]\n",
    "    return keywords\n",
    "\n",
    "def gather_data(api_key1, api_key2, api_key3, keywords, side_ed):\n",
    "    \"\"\"\n",
    "    Gather data from YouTube API for a given set of keywords.\n",
    "\n",
    "    Parameters:\n",
    "    - api_key (str): YouTube Data API keys.\n",
    "    - keywords (list): A list of keywords.\n",
    "    - side_ed (int): 0 if con-ed, 1 in pro-ed contant scraped.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Lists of video titles, descriptions, video IDs, and categories.\n",
    "    \"\"\"\n",
    "    youtube_api = build('youtube', 'v3', developerKey=api_key1)\n",
    "    titles = []\n",
    "    descriptions = []\n",
    "    comments = []\n",
    "    ids = []\n",
    "    side = []\n",
    "    info = []\n",
    "    markup = []\n",
    "\n",
    "    for keyword in keywords:\n",
    "        req = youtube_api.search().list(q=keyword, part='snippet', type='video', maxResults=maxResults)\n",
    "        res = req.execute()\n",
    "        index = keywords.index(keyword) + 1\n",
    "\n",
    "        while len(titles) < (maxResults * index):\n",
    "            for i in range(len(res['items'])):\n",
    "                titles.append(res['items'][i]['snippet']['title'])\n",
    "                descriptions.append(get_full_desc(res['items'][i]['id']['videoId'], api_key2))\n",
    "                comments.append(get_comments(res['items'][i]['id']['videoId'], api_key3))\n",
    "                ids.append(res['items'][i]['id']['videoId'])\n",
    "                category.append(keyword)\n",
    "\n",
    "                side.append(side_ed)\n",
    "                info.append(1-side_ed) #assumed all con-ed were informative, all pro-ed non-informative\n",
    "                markup.append(side_ed) # all pro-ed contained markup-words like tw and recovery help (misleading terms), all con-ed did not\n",
    "\n",
    "            if 'nextPageToken' in res:\n",
    "                next_page_token = res['nextPageToken']\n",
    "                req = youtube_api.search().list(q=keyword, part='snippet', type='video', maxResults=maxResults,\n",
    "                                                pageToken=next_page_token)\n",
    "                res = req.execute()\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    return titles, descriptions, ids, side, info, markup, comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae37574-89f3-46bf-a052-a441d4e0306e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bro if you keep like this you are going to die, but is that what you want right?  Iâ€™m in recovery atm and I would be even worse than this, I would eat 1 meal a day with no snacks and that meals was a\n"
     ]
    }
   ],
   "source": [
    "comment1 = get_comments(\"0s6ohWO6YXY\",API_KEY1)\n",
    "print(comment1[:200]) #prints all comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1b8fc91-2bbd-4494-a968-b8f5816115c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "comment2 = get_comments(\"FEY9miq9wkY\",API_KEY1)\n",
    "print(comment2) #video where comments are disabled (kids content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e241098d-4bdb-45ee-9bf1-cc8c9994ca2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "comment3 = get_comments(\"8rZP8ef7fps\",API_KEY1)\n",
    "print(comment3) #video with no comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "088fb034-927f-440c-9c5d-876c7938ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read keywords, store in list\n",
    "pro_keywords = read_keywords_from_file(\"pro_ed_labels.txt\")\n",
    "con_keywords = read_keywords_from_file(\"con_ed_labels.txt\")\n",
    "\n",
    "#step 1: pro-ed data with 1 api key\n",
    "pro_titles, pro_descriptions, pro_ids, pro_side, pro_info, pro_markup, pro_comments = gather_data(API_KEY1, API_KEY2, API_KEY3, pro_keywords, 1)\n",
    "\n",
    "#step 2: con-ed data with other api key\n",
    "con_titles, con_descriptions, con_ids, con_side, con_info, con_markup, con_comments = gather_data(API_KEY4, API_KEY5, API_KEY6, con_keywords, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7181d99f-ebf7-4b19-a2b8-58cf76523e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1303 1303 1303 1303 1303 1303 1303\n"
     ]
    }
   ],
   "source": [
    "# Construct Dataset\n",
    "final_titles = pro_titles + con_titles\n",
    "final_descriptions = pro_descriptions + con_descriptions\n",
    "final_comments = pro_comments + con_comments\n",
    "final_ids = pro_ids + con_ids\n",
    "side = pro_side + con_side\n",
    "info = pro_info + con_info\n",
    "markup = pro_markup + con_markup\n",
    "\n",
    "print(len(final_titles), len(final_descriptions), len(final_comments), len(final_ids), len(side), len(info), len(markup))\n",
    "data = pd.DataFrame({'Video Id': final_ids, 'Title': final_titles, 'Description': final_descriptions, 'Comments' : final_comments,\n",
    "                     'Query': category, 'Pro_or_Con': side, 'Informative': info, 'Markup_terms': markup})\n",
    "data.to_csv('raw_data_with_comments.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127d2677-5b86-417a-8e05-3a1599c5261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to try and perform sentiment analysis using comments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
